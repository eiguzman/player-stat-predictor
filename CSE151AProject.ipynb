{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QsmR09N9zoo"
   },
   "source": [
    "Our project looks to examine the key factors influencing NBA player scoring, utilizing a comprehensive dataset of player statistics from 1950 to 2017. By examining a wide range of features from our dataset—including player position, minutes played, field goal percentage, and usage rate—we seek to identify the elements that contribute most significantly to a player's scoring ability, and to be able to predict future stats with our supervised learning model (supervised because we have quantitative labeled data). In addition, we want to know which combinations of these features are the best predictors for high point totals in an NBA player's entire career. The insights gained from this study can be a valuable resource for sports bettors, fantasy basketball players, and even the coaches or GMs who manage the team themselves when trying to pick players with the best scoring potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6BOX3YHkvfpm",
    "outputId": "521dbb25-7918-4e93-86d8-bf47b7caa587"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/jonathanaduong/CSE151AGroupProject.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd4t1wNIzyGH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "mWdieT8zz4Im",
    "outputId": "8339f842-f52e-48b1-eaa3-486a37ac2dd7"
   },
   "outputs": [],
   "source": [
    "nbadf = pd.read_csv('CSE151AGroupProject/Seasons_Stats.csv')\n",
    "nbadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "k0SKuWK60E9i",
    "outputId": "6bd666aa-85e5-4665-ae6d-b80e844a3ed3"
   },
   "outputs": [],
   "source": [
    "nbadf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yGxsVd-06Nr",
    "outputId": "a6f5ee6d-a7c9-4745-a3f7-d69cef8f5566"
   },
   "outputs": [],
   "source": [
    "print(nbadf.Pos.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "ky4tPRjMbQyR",
    "outputId": "121a8ae7-4bfa-426d-c2fd-71bf5c6a9882"
   },
   "outputs": [],
   "source": [
    "nbadf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQj-VOfUhf_w",
    "outputId": "bf9f3a5d-1601-484a-e8c3-1adce55174e3"
   },
   "outputs": [],
   "source": [
    "print(nbadf.Tm.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "id": "Imsz26Jtiwrv",
    "outputId": "e4446af6-2e7e-42db-db83-196f061fbcc5"
   },
   "outputs": [],
   "source": [
    "team_counts = nbadf['Tm'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "team_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Frequency of People in Each Team')\n",
    "plt.xlabel('Team')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bDQSSn2dcyHl",
    "outputId": "37fea5dd-ceab-4200-9693-306830c3f635"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(nbadf[['PTS', 'G', 'MP', 'FG%', 'FGA', 'FT%', 'FTA']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "gGJID87Yc02y",
    "outputId": "1b3f3f4b-6d8d-41c6-a043-92454e8adde1"
   },
   "outputs": [],
   "source": [
    "corr = nbadf[['PTS', 'G', 'MP', 'FG%', 'FGA', 'FT%', 'FTA']].corr()\n",
    "sns.heatmap(corr, vmin=-1, vmax=1, center=0, annot=True, cmap= 'RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vc8iD3m3mPrD",
    "outputId": "3c684a92-e2b8-4061-ab41-dc833e7227ff"
   },
   "outputs": [],
   "source": [
    "nbadf.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7T1ugQPOaczB"
   },
   "source": [
    "The data set tracks players' statistics season to season from 1950-2017. The categorical features we plan on observing are Pos (Player position) and Tm (Team they played for) and the numerical features that we plan to observe are G (Games played), MP (Total minutes played), FG% (Percentage hots made in a season), FGA (Shots attempted in a season), FT% (Percentage of free throws made in a season), and FTA (Free throws attempted in a season).\n",
    "\n",
    "We plan on predicting the points scored per game for an individual, so we need to observe the games played each season for a player (G) as FGA, FTA, and MP are the totals for points scored, field goals shot, free throws shot, and minutes played for the season rather than a per game average, which we would prefer to analyze. We will also be seeing if player position makes a difference in the point scored per game.\n",
    "\n",
    "The data set has 24,691 different observations, although there are repeat players since they played across multiple years. For scale, we plan to use minMax scaling to ensure that the data is normalized. There are many null/NaN values across seasons since these statistics were not recorded at the time, such as blocks or steals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOiCLtzcjjUL"
   },
   "source": [
    "### How will you preprocess your data :\n",
    "- to (one-hot) encode categorical data like Team, Positions\n",
    "- for null/nan values, we will replace it with an adjusted mean values\n",
    "- for each of the season total categories (PTS, TRB, AST), we will divide it by the games played to get the per game average.\n",
    "- use min-max to scale since we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "VVbR_nnts5Dg",
    "outputId": "8ef7ccdd-b54f-48d7-eb7f-e89cc2a77d1f"
   },
   "outputs": [],
   "source": [
    "processed_nbadf = nbadf.copy()\n",
    "removeFeat =  ['Unnamed: 0', 'blanl' , 'blank2', 'GS', 'Player', 'ORB', 'DRB', '3P', '2P', 'FG']\n",
    "processed_nbadf = processed_nbadf.drop(removeFeat, axis=1)\n",
    "\n",
    "#fill NaNs in each non-category column\n",
    "for i in processed_nbadf.columns:\n",
    "  if i == 'Year': continue\n",
    "  if i == 'MP': continue\n",
    "  if processed_nbadf[i].dtype == 'object': continue\n",
    "  #by mean imputation grouped by year:\n",
    "  processed_nbadf[i] = processed_nbadf.groupby('Year')[i].transform(lambda x: x.fillna(x.mean()))\n",
    "  col_mean = processed_nbadf[i].mean()\n",
    "  #by global average:\n",
    "  processed_nbadf[i] = processed_nbadf[i].fillna(col_mean)\n",
    "\n",
    "#compute average MP per game\n",
    "avg_MP = processed_nbadf[\"MP\"].mean()\n",
    "avg_G = processed_nbadf[\"G\"].mean()\n",
    "processed_nbadf['MP_per_game'] = processed_nbadf['MP'] / processed_nbadf['G']\n",
    "#fill NaNs with the global average\n",
    "processed_nbadf['MP_per_game'] = processed_nbadf['MP_per_game'].fillna(avg_MP / avg_G)\n",
    "\n",
    "#remove remaining NaNs\n",
    "processed_nbadf.dropna(inplace= True)\n",
    "\n",
    "# #one-hot encode categories\n",
    "processed_nbadf = pd.get_dummies(processed_nbadf, columns=['Tm', 'Pos'])\n",
    "processed_nbadf\n",
    "\n",
    "#compute per game and fill out na values with mean\n",
    "feature_cols = []\n",
    "for column in ['PTS', 'TRB', 'AST', 'MP', 'FGA', '3PA', '2PA', 'FTA']:\n",
    "  col = column + '_per_game'\n",
    "  feature_cols.append(col)\n",
    "  processed_nbadf[col] = processed_nbadf[column] / processed_nbadf['G']\n",
    "  processed_nbadf.drop(column, axis = 1)\n",
    "\n",
    "processed_nbadf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VxtkfaN88VM"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMGehieM66na"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lf7o8f6MdY-m"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F758NS_U4UOv"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(processed_nbadf[['TRB_per_game', 'AST_per_game', 'MP_per_game', 'FGA_per_game', '3PA_per_game', '2PA_per_game', 'FTA_per_game']], processed_nbadf.PTS_per_game, test_size=0.2, random_state=21)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bRuIjX4MkZy",
    "outputId": "978f1496-1fcd-4d65-a8ce-ff1fb043eae4"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape[0]/processed_nbadf.shape[0])\n",
    "print(X_test.shape[0]/processed_nbadf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qM370jE6XjtO",
    "outputId": "28330940-107a-49cf-a8a7-e55187b51b06"
   },
   "outputs": [],
   "source": [
    "for i in ['TRB_per_game', 'AST_per_game', 'MP_per_game', 'FGA_per_game', '3PA_per_game', '2PA_per_game', 'FTA_per_game']:\n",
    "  linearreg = LinearRegression()\n",
    "  linearmodel = linearreg.fit(X_train[i].values.reshape(-1,1), y_train)\n",
    "  yhat_test = linearreg.predict(X_test[i].values.reshape(-1,1))\n",
    "  yhat_train = linearreg.predict(X_train[i].values.reshape(-1,1))\n",
    "  print('Testing MSE: %.2f' % mean_squared_error(y_test, yhat_test))\n",
    "  plt.scatter(X_train[i],y_train, s=10)\n",
    "  plt.xlabel(i)\n",
    "  plt.ylabel('PTS_per_game')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FTie4VBCj0cP",
    "outputId": "eb0e86e3-e2d0-4e55-df3e-a7d9704475f4"
   },
   "outputs": [],
   "source": [
    "  linearreg = LinearRegression()\n",
    "  ## y_train and y_test serve as ground truth labels, representing the actual points per game derived from our dataset. These values were calculated by dividing each player’s total points in the season by the number of games played, providing a reliable basis for evaluating the model's predictions.\n",
    "  linearmodel = linearreg.fit(X_train, y_train)\n",
    "  print(linearmodel.coef_)\n",
    "  yhat_test = linearreg.predict(X_test)\n",
    "  yhat_train = linearreg.predict(X_train)\n",
    "  print('Testing MSE: %.2f' % mean_squared_error(y_test, yhat_test))\n",
    "  print('Training MSE: %.2f' % mean_squared_error(y_train, yhat_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTYk2WK3mcYO"
   },
   "source": [
    "## Conclusion\n",
    "The conclusion of our 1st model shows our model can achieve a moderate level of accuracy in predicting PPG. To improve our model in the future, we might want to exclude most of the data from 1950s to 1980s because lots of the data had to be imputed and are carried by global averages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ4zuROttCf7"
   },
   "source": [
    "(dionne's attempt code)\n",
    "- instead of hyperparameter tuning maybe we can try GD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 772
    },
    "id": "9NyzqKpRqxSh",
    "outputId": "a8aac11e-a57c-4b87-fe04-e0a2f5468ba6"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_nbadf[['TRB_per_game', 'AST_per_game', 'MP_per_game', 'FGA_per_game', '3PA_per_game', '2PA_per_game', 'FTA_per_game','WS','BPM', 'PER']], processed_nbadf.PTS_per_game, test_size=0.2, random_state=21)\n",
    "\n",
    "# min max scale\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# turn X_train and X_test into\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "## for the fitting graph\n",
    "train_mse_list = []\n",
    "test_mse_list = []\n",
    "\n",
    "# polynomial degree to add complexity to the model\n",
    "degrees = [1,2,3,4]\n",
    "for degree in degrees:\n",
    "  initialDegree = degree\n",
    "  poly = PolynomialFeatures(degree=initialDegree)\n",
    "  X_train_poly = poly.fit_transform(X_train)\n",
    "  X_test_poly = poly.transform(X_test)\n",
    "\n",
    "  model2 = LinearRegression()\n",
    "  model2.fit(X_train_poly, y_train)\n",
    "  y_train_pred = model2.predict(X_train_poly)\n",
    "  y_test_pred = model2.predict(X_test_poly)\n",
    "\n",
    "\n",
    "  train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "  test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "  train_mse_list.append(train_mse)\n",
    "  test_mse_list.append(test_mse)\n",
    "\n",
    "  print(f\"Polynomial Regression (Degree={initialDegree})\")\n",
    "  print(f\"Train MSE: {train_mse:.2f}\")\n",
    "  print(f\"Test MSE: {test_mse:.2f}\")\n",
    "\n",
    "## WILL CHOOSE Degree 3 for Polynomial Regression, better performance with\n",
    "## least signs of overfitting\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, train_mse_list, marker='o', label='Train MSE')\n",
    "plt.plot(degrees, test_mse_list, marker='o', label='Test MSE')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Model Fitting: Train vs Test MSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cV3be1_7xvnx"
   },
   "source": [
    "3. Our model lies in the middle of this fitting graph, where degree equals 3. A decently complex model, which makes sure we are not underfitting while also minimizing the difference between our training and testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kG16QuGUWXy5",
    "outputId": "160e5f18-5ba1-4105-db5a-b86e4af4b6a0"
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=3)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X_train_poly, y_train)\n",
    "y_test_pred = model2.predict(X_test_poly)\n",
    "y_test_pred\n",
    "threshold = 1\n",
    "newY = y_test - y_test_pred\n",
    "\n",
    "## in the context of this question any Correct is ppg predicted within .5 of the true values\n",
    "## while FN and FP are calculated as any values not within .5 of the true values\n",
    "correct = len(newY[abs(newY) < .5])\n",
    "FNFP = len(y_test) - correct\n",
    "print(f'Correct Values: ', correct)\n",
    "print(f'FNFP Values: ', FNFP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Model 3\n",
    "For our third model, we will predict a player's **Points Per Game** as a **Category**. Specifically, we will bin PPG as follows:\n",
    "\n",
    "|Bin|Score|\n",
    "|----------|-------|\n",
    "|PPG <= 25%|\"Low\"|\n",
    "|25% < PPG <= 50%|\"Medium\"|\n",
    "|50% < PPG <= 75%|\"High\"|\n",
    "|PPG > 75%|\"Very High\"|\n",
    "\n",
    "The thresholds are printed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['PTS_per_game']\n",
    "#ppg mean is 8.3; max is 50\n",
    "desc = processed_nbadf[cols].describe()\n",
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform additional preprocessing for this version of our data by adding a new column into our modified DataFrame called \"PPG Bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = {\"Low\": desc[\"PTS_per_game\"][\"25%\"], \"Medium\": desc[\"PTS_per_game\"][\"50%\"], \"High\": desc[\"PTS_per_game\"][\"75%\"]}\n",
    "print(bins)\n",
    "nbadf_2 = processed_nbadf.copy()\n",
    "\n",
    "#Convert keys into numeric vals, Very High = 3\n",
    "def PPG_binner(value):\n",
    "    if value < bins['Low']:\n",
    "        return 0\n",
    "    elif value < bins['Medium']:\n",
    "        return 1\n",
    "    elif value < bins['High']:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "#Apply the function to PTS_per_game\n",
    "nbadf_2['PPG_Bins'] = nbadf_2['PTS_per_game'].apply(PPG_binner)\n",
    "nbadf_2 = nbadf_2.drop(\"PTS_per_game\", axis=1)\n",
    "#Drop the PTS_per_game column\n",
    "nbadf_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform either PCA or SVD using the same dimensions as we did in our previous model. \n",
    "\n",
    "We will not take into account the position of a player. This is because PCA using positions results in 5 clusters, binned most likely by the 5 main positions in a basketball team. Within those clusters, observations are clustered by PPG bins. A graph of the results is seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    nbadf_2.iloc[:, -7:-1], \n",
    "    nbadf_2.PPG_Bins, \n",
    "    test_size=0.2, \n",
    "    random_state=21)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "\n",
    "ev = pca.explained_variance_ratio_\n",
    "\n",
    "# Scree plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(ev) + 1), ev, marker='o', linestyle='--')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xticks(np.arange(1, len(ev) + 1, 1))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Optionally, you can also plot the cumulative explained variance\n",
    "cumulative_variance = np.cumsum(ev)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.title('Cumulative Explained Variance Plot')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.xticks(np.arange(1, len(cumulative_variance) + 1, 1))\n",
    "plt.grid()\n",
    "plt.axhline(y=0.90, color='r', linestyle='--')  # Example threshold at 90%\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our PCA will perform best when using 3 components, as seen by the plots above. SVD Could have also been used to determine the number of components, as we have previously done in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components= 3)\n",
    "pca_df = pd.DataFrame(pca.fit_transform(X_train))\n",
    "pca_df['Labels'] = nbadf_2.PPG_Bins\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(pca_df[0], pca_df[1],\n",
    "            c=pca_df['Labels'],\n",
    "            alpha=0.6)\n",
    "plt.title('PCA Clusters')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 0\n",
    "hi_feat = -1\n",
    "hi_weight = -1\n",
    "for i in pca.components_[0]:\n",
    "    if i > hi_weight and i > 0:\n",
    "        hi_feat = feature\n",
    "        hi_weight = i\n",
    "    feature += 1\n",
    "\n",
    "print('Feature with the heighest positive weight:')\n",
    "print(hi_feat, hi_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset results in poor PCA clustering for a few reasons.\n",
    "First, our PCA graph has a wedge shape due to the abscence of negative values. This can also be seen by plotting a histogram of PPG; our data is right skewed as more players have small points scored per game, while a few \"Power Forwards\" score the bulk of a team's points. The second reason is most likely due to the standardization performed on each variable prior to PCA.\n",
    "As such, we will be using SVD with 3 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=3, n_iter=1000, random_state=76)\n",
    "svd.fit(X_train)\n",
    "sv = svd.singular_values_\n",
    "\n",
    "right_matrix = pd.DataFrame(svd.components_)\n",
    "right_matrix.shape # lets check the shape\n",
    "\n",
    "left_matrix = pd.DataFrame(svd.fit_transform(X_train))/ sv\n",
    "left_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize what our SVD looks like, we will use a pairplot showing each of the 3 components of the left and right matrices we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def generate_plot(svd):\n",
    "    sv = svd.singular_values_\n",
    "    left_matrix = pd.DataFrame(svd.transform(X_train)) / sv\n",
    "    right_matrix = pd.DataFrame(svd.components_)\n",
    "    \n",
    "    # Left matrix pairplot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.pairplot(left_matrix)\n",
    "    plt.suptitle(\"Pairplot of Left Matrix (Transformed Features)\", y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Right matrix pairplot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.pairplot(right_matrix.T)\n",
    "    plt.suptitle(\"Pairplot of Right Matrix (Components)\", y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plot(svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see why our PCA did not produce good clusters. Our vertical plots of n=2 and n=3 show a unmimodal distribution. Had our data shown multiple peaks, clustering would have become more noticeable.\n",
    "\n",
    "Due to the small amount of observations in our right matrix, very little significant information is available that the left matrix is does not already produce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can perform logistic regression by fitting a regressor on our transformed training data, predicting our test data, and comparing our resulting predictions with our actual y values using classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Step 1: Transform both training and testing data using SVD\n",
    "X_train_svd = pd.DataFrame(svd.transform(X_train), index=X_train.index)\n",
    "X_test_svd = pd.DataFrame(svd.transform(X_test), index=X_test.index)\n",
    "\n",
    "# Step 2: Fit a classifier on the transformed training data\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_svd, y_train)\n",
    "\n",
    "# Step 3: Predict on the transformed test data\n",
    "y_pred = classifier.predict(X_test_svd)\n",
    "\n",
    "# Optional: Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test accuracy hovers around roughly 84%. We would need to run these models again without seeding to determine its true efficacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = classifier.predict(X_train_svd)\n",
    "\n",
    "# Optional: Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred2))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_train, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the prediction on our training data also produces a similar accuracy score. This is indicative of the SVD improving our predictive probability when classifying Points Per Game as a categorical variable. Should we increase the bins (for example, from quartiles to deciles), our accuracy would decrease slightly, but it would provide a significantly better predictor in comparison to running linear regression on the normalized PPG."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
